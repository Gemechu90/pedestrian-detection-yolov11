# üö∂ YOLO Pedestrian Detection with CBAM Attention

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![YOLOv11](https://img.shields.io/badge/YOLOv11-Medium-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**A state-of-the-art pedestrian detection system using YOLOv11 enhanced with CBAM attention mechanisms**

[Features](#-features) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Results](#-results) ‚Ä¢ [Architecture](#-architecture) ‚Ä¢ [Dataset](#-dataset)

</div>

---

## üìã Overview

This project implements an advanced pedestrian detection system using **YOLOv11-Medium** as the base architecture, enhanced with **Convolutional Block Attention Module (CBAM)** for improved feature extraction and detection accuracy. The model is trained on the **Caltech Pedestrian Dataset**, one of the most challenging benchmarks for pedestrian detection.

### ‚ú® Key Highlights

- üéØ **Enhanced YOLOv11**: Custom architecture with CBAM attention mechanisms
- üî• **CBAM Integration**: Channel and spatial attention for better feature representation
- üìä **Comprehensive Training**: 50 epochs with batch size optimization
- üé® **Visualization Tools**: Advanced tracking and detection visualization
- üìà **Performance Metrics**: Detailed precision, recall, mAP tracking
- üöÄ **Production Ready**: Optimized for real-time inference

---

## üåü Features

### Core Capabilities
- ‚úÖ Real-time pedestrian detection in images and videos
- ‚úÖ Custom YOLOv11-CBAM architecture with attention mechanisms
- ‚úÖ Multi-scale feature extraction and fusion
- ‚úÖ Comprehensive evaluation metrics (Precision, Recall, mAP50, mAP50-95)
- ‚úÖ Confidence threshold filtering (‚â•0.5)
- ‚úÖ Visualization of detection results with bounding boxes

### Technical Features
- **CBAM Attention Module**: Dual attention mechanism (channel + spatial)
- **Transfer Learning**: Leverages pretrained YOLOv11-Medium weights
- **Data Augmentation**: Built-in YOLO augmentation pipeline
- **Custom Dataset Pipeline**: Automated Caltech dataset processing
- **Performance Tracking**: Real-time metrics visualization with Plotly

---

## üèóÔ∏è Architecture

### YOLOv11-CBAM Model

The model integrates CBAM attention modules into the YOLOv11-Medium backbone at strategic layers (layers 2 and 4) to enhance feature representation:

```
Input Image (640√ó480)
    ‚Üì
YOLOv11 Backbone
    ‚Üì
Layer 2 + CBAM ‚Üí Enhanced Features
    ‚Üì
Layer 4 + CBAM ‚Üí Enhanced Features
    ‚Üì
YOLOv11 Neck (PANet)
    ‚Üì
YOLOv11 Head
    ‚Üì
Detection Output
```

### CBAM Attention Mechanism

```python
class CBAM(nn.Module):
    """Convolutional Block Attention Module"""
    - Channel Attention: avg_pool + max_pool ‚Üí FC layers ‚Üí attention weights
    - Spatial Attention: channel-wise pooling ‚Üí conv ‚Üí attention map
    - Output: input_features √ó channel_attention √ó spatial_attention
```

**Benefits**:
- Focuses on informative features while suppressing irrelevant ones
- Improves detection of pedestrians at various scales
- Enhances performance in challenging scenarios (occlusion, small objects)

---

## üìä Dataset

### Caltech Pedestrian Dataset

The model is trained on the **Caltech Pedestrian Detection Benchmark**, which contains:

- **Training Set**: Multiple video sequences from set00-set10
- **Validation Set**: Separate test sequences
- **Image Size**: 640√ó480 pixels
- **Annotations**: Bounding boxes in YOLO format (normalized xywh)
- **Class**: Single class - `person`

#### Dataset Structure
```
datasets/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ caltechpedestriandataset/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ set00/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ set01/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îÇ       ‚îî‚îÄ‚îÄ caltechpedestriandataset/
‚îî‚îÄ‚îÄ labels/
    ‚îú‚îÄ‚îÄ train/
    ‚îî‚îÄ‚îÄ val/
```

#### Preprocessing Pipeline
1. **Annotation Conversion**: Matlab annotations ‚Üí YOLO format
2. **Box Format**: `(x_center, y_center, width, height)` normalized to [0, 1]
3. **Filtering**: Remove occluded and partially visible pedestrians
4. **Frame Sampling**: Strategic frame selection from video sequences

---

## üöÄ Installation

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended)
- 8GB+ RAM

### Setup

```bash
# Clone the repository
git clone https://github.com/Gemechu90/yolo-pedestrian-detection.git
cd yolo-pedestrian-detection

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Requirements

```txt
ultralytics>=8.0.0
torch>=2.0.0
torchvision>=0.15.0
opencv-python>=4.8.0
numpy>=1.24.0
pandas>=2.0.0
scipy>=1.10.0
plotly>=5.14.0
tqdm>=4.65.0
matplotlib>=3.7.0
```

---

## üíª Usage

### 1. Prepare the Dataset

```python
# The notebook includes automated dataset preparation
# Converts Caltech annotations to YOLO format
python prepare_dataset.py
```

### 2. Train the Model

#### Standard YOLOv11-Medium
```python
from ultralytics import YOLO

model = YOLO("yolo11m.pt")
model.train(
    data="custom_dataset.yaml",
    epochs=50,
    batch=32,
    verbose=True
)
```

#### YOLOv11-CBAM (Enhanced)
```python
from custom_ultralytics.models import YOLO11m_CBAM

# Initialize custom model
cbam_model = YOLO11m_CBAM()

# Load pretrained weights
base_model = YOLO("yolo11m.pt")
cbam_model.load_pretrained(base_model)

# Train
cbam_model.train(
    data="custom_dataset.yaml",
    epochs=50,
    batch=32
)
```

### 3. Run Inference

```python
from ultralytics import YOLO

# Load trained model
model = YOLO('runs/detect/train/weights/best.pt')

# Predict on images
results = model.predict(
    source='path/to/images',
    conf=0.5,  # Confidence threshold
    save=True   # Save results
)

# Predict on video
results = model.predict(
    source='path/to/video.mp4',
    conf=0.5,
    save=True
)
```

### 4. Evaluate Performance

```python
# Validation
metrics = model.val()

print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")
print(f"Precision: {metrics.box.p}")
print(f"Recall: {metrics.box.r}")
```

---

## üìà Results

### Performance Metrics

| Metric | Value |
|--------|-------|
| **Precision** | Tracked across 50 epochs |
| **Recall** | Tracked across 50 epochs |
| **mAP@0.5** | Tracked across 50 epochs |
| **mAP@0.5:0.95** | Tracked across 50 epochs |
| **Inference Speed** | Real-time capable |

### Training Configuration
- **Base Model**: YOLOv11-Medium
- **Epochs**: 50
- **Batch Size**: 32
- **Image Size**: 640√ó480 (native Caltech resolution)
- **Optimizer**: AdamW (default YOLO)
- **Learning Rate**: Auto-scheduled

### Visualization Examples

The notebook includes comprehensive visualization tools:
- ‚úÖ Training/validation loss curves
- ‚úÖ Precision-Recall curves
- ‚úÖ mAP progression over epochs
- ‚úÖ Detection results with bounding boxes
- ‚úÖ Frame-by-frame tracking sequences

---

## üî¨ Model Architecture Details

### CBAM Module Implementation

```python
class CBAM(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        # Channel Attention
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1, bias=False)
        )
        
        # Spatial Attention
        self.spatial = nn.Sequential(
            nn.Conv2d(2, 1, 7, padding=3, bias=False),
            nn.Sigmoid()
        )
```

### Integration Strategy

1. **Layer Selection**: CBAM modules added after layers 2 and 4
2. **Channel Reduction**: 16√ó reduction ratio for efficiency
3. **Activation**: Sigmoid for attention weights
4. **Sequential Integration**: Maintains gradient flow

---

## üìÅ Project Structure

```
yolo-pedestrian-detection/
‚îú‚îÄ‚îÄ custom_ultralytics/
‚îÇ   ‚îú‚îÄ‚îÄ nn/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ cbam.py          # CBAM implementation
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ yolo11m_cbam.py      # Custom YOLO model
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ       ‚îú‚îÄ‚îÄ train/
‚îÇ       ‚îî‚îÄ‚îÄ val/
‚îú‚îÄ‚îÄ runs/
‚îÇ   ‚îî‚îÄ‚îÄ detect/
‚îÇ       ‚îî‚îÄ‚îÄ train/
‚îÇ           ‚îî‚îÄ‚îÄ weights/
‚îÇ               ‚îú‚îÄ‚îÄ best.pt      # Best model weights
‚îÇ               ‚îî‚îÄ‚îÄ last.pt      # Last checkpoint
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ yolo-pedestrian.ipynb    # Main training notebook
‚îú‚îÄ‚îÄ custom_dataset.yaml          # Dataset configuration
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üõ†Ô∏è Advanced Features

### Custom Dataset Configuration

```yaml
# custom_dataset.yaml
path: /path/to/datasets
train: /path/to/datasets/images/train
val: /path/to/datasets/images/val

nc: 1  # number of classes
names:
  0: person
```

### Detection Pipeline

```python
def detect_people(frame_list):
    """
    Detects pedestrians in a list of frames
    
    Args:
        frame_list: List of image file paths
        
    Returns:
        all_box_list: List of bounding boxes per frame
        all_conf_list: List of confidence scores per frame
    """
    model = YOLO('runs/detect/train/weights/best.pt')
    results = model.predict(frame_list, verbose=False)
    
    all_boxes = []
    all_confs = []
    
    for result in results:
        boxes = result.boxes
        frame_boxes = []
        frame_confs = []
        
        for box in boxes:
            if box.conf >= 0.5:  # Confidence threshold
                frame_boxes.append(box.xyxy[0].cpu().numpy())
                frame_confs.append(float(box.conf))
        
        all_boxes.append(frame_boxes)
        all_confs.append(frame_confs)
    
    return all_boxes, all_confs
```

---

## üéØ Use Cases

- **Autonomous Vehicles**: Pedestrian detection for self-driving cars
- **Surveillance Systems**: Crowd monitoring and tracking
- **Smart Cities**: Traffic analysis and pedestrian flow management
- **Retail Analytics**: Customer counting and behavior analysis
- **Safety Applications**: Construction site monitoring, crosswalk safety

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guide
- Add unit tests for new features
- Update documentation as needed
- Ensure all tests pass before submitting PR

---

## üìù Citation

If you use this project in your research, please cite:

```bibtex
@misc{yolo-pedestrian-cbam,
  author = {Your Name},
  title = {YOLO Pedestrian Detection with CBAM Attention},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/yourusername/yolo-pedestrian-detection}
}
```

### References
- **YOLOv11**: [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
- **CBAM**: [Woo et al., "CBAM: Convolutional Block Attention Module", ECCV 2018](https://arxiv.org/abs/1807.06521)
- **Caltech Dataset**: [Doll√°r et al., "Pedestrian Detection: An Evaluation of the State of the Art", PAMI 2012](https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Ultralytics** for the excellent YOLOv11 implementation
- **Caltech Vision Lab** for the pedestrian detection benchmark
- **PyTorch Team** for the deep learning framework
- **CBAM Authors** for the attention mechanism architecture

---

## üìß Contact

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

---

## üîÑ Updates & Roadmap

### Current Version: 1.0.0

### Planned Features
- [ ] Multi-class pedestrian detection (walking, running, standing)
- [ ] Real-time video stream processing
- [ ] Model quantization for edge deployment
- [ ] Integration with ROS for robotics applications
- [ ] Web interface for easy inference
- [ ] TensorRT optimization for NVIDIA GPUs
- [ ] ONNX export for cross-platform deployment

---

<div align="center">

**‚≠ê Star this repository if you find it helpful!**

Made with ‚ù§Ô∏è by [Gemechu Geleta]

</div>
